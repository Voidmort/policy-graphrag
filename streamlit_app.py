import asyncio
import hashlib
import random
import streamlit as st
import torch
import os

from torch import classes  # æ˜¾å¼å¯¼å…¥ç±»æ³¨å†Œæ¨¡å—

torch.classes.__path__ = []

from traceback import print_exception
from streamlit_agraph import agraph, Node, Edge, Config
from typing import Literal
from collections import defaultdict
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ï¼ˆé»˜è®¤ä»å½“å‰ç›®å½•åŠ è½½ï¼‰
load_dotenv(".env")

from policy_graphrag.data_model.query import QueryParam
from policy_graphrag.embeddings import OpenaiEmbedding
from policy_graphrag.llms import OpenAIProvider
from policy_graphrag import PolicyGraphRAG


# --- Streamlit é…ç½® ---
st.set_page_config(page_title="æ”¿ç­–é—®ç­”åŠ©æ‰‹", layout="wide", menu_items=None)

st.markdown(
    '<h1 style="text-align: center; font-size: 42px;">ğŸ“˜ æ”¿ç­–é—®ç­”åŠ©æ‰‹</h1>',
    unsafe_allow_html=True,
)


@st.cache_resource
def load_question_list():
    return [
        "å“ªäº›æ”¿ç­–æ–‡ä»¶ç”±æ•™è‚²ä¿¡æ¯åŒ–éƒ¨é—¨è´Ÿè´£æŠ€æœ¯æ”¯æŒä¸ç³»ç»Ÿè¿è¡Œä¿éšœï¼Ÿ",
        "å“ªäº›æ”¿ç­–æ–‡ä»¶æ¶‰åŠé«˜æ ¡è¾…å¯¼å‘˜åŸ¹è®­å’Œç ”ä¿®åŸºåœ°çš„å¤‡æ¡ˆè¯„ä¼°åŠè¾…å¯¼å‘˜ä¸“ä¸šæŠ€æœ¯èŒåŠ¡è˜ä»»æƒ…å†µï¼Ÿ",
        "å“ªäº›æ”¿ç­–æ–‡ä»¶æ¶‰åŠéæŠ˜å çº¸ç›’ç±»åŒ…è£…è£…æ½¢å°åˆ·å“å°åˆ¶è´¨é‡çš„åˆ¤å®šæ ‡å‡†ï¼Ÿ",
        "å“ªäº›æ”¿ç­–æ–‡ä»¶æ¶‰åŠèŒä¸šæ•™è‚²è¯¾ç¨‹æ€æ”¿å»ºè®¾å¹¶åŒ…å«è¯¾ç¨‹æ€æ”¿ç¤ºèŒƒé¡¹ç›®ï¼Ÿ",
        "å“ªäº›æ”¿ç­–æ–‡ä»¶æ¶‰åŠè€ƒç”Ÿè€ƒè¯•æŠ¥åæ—¶é—´çš„å®‰æ’ä»¥åŠç›¸å…³çš„è€ƒè¯•é€šçŸ¥ï¼Ÿ",
        "å“ªäº›æ”¿ç­–æ–‡ä»¶æ¶‰åŠç¬¬å…«å±Šå…¨å›½é’å°‘å¹´æ°‘æ—å™¨ä¹æ•™è‚²æ•™å­¦æˆæœå±•ç¤ºæ´»åŠ¨çš„æˆªæ­¢æ—¶é—´åŠç›¸å…³ç®¡ç†è§„å®šï¼Ÿ",
        "å“ªäº›æ”¿ç­–æ–‡ä»¶æ¶‰åŠçŸ¥è¯†äº§æƒä¿æŠ¤å¹¶è¦æ±‚å­¦å‘˜å°Šé‡æˆè¯¾è®²å¸ˆç›¸å…³èµ„æ–™çš„çŸ¥è¯†äº§æƒï¼Ÿ",
        "å“ªäº›æ”¿ç­–æ–‡ä»¶æ¶‰åŠæ ¡å›­å®‰å…¨å«ç”Ÿå’Œç”Ÿå‘½å®‰å…¨ä¸å¥åº·æ•™è‚²è¿›ä¸­å°å­¦è¯¾ç¨‹æ•™ææŒ‡å—çš„å…³ç³»ï¼Ÿ",
        "å“ªäº›æ”¿ç­–æ–‡ä»¶æ¶‰åŠæ•™è‚²éƒ¨èŒä¸šé™¢æ ¡æ•™å­¦æŒ‡å¯¼å§”å‘˜ä¼šå§”å‘˜çš„ç»„ç»‡æ¨èåŠä¸»ä»»å§”å‘˜å•ä½çš„ç¡®å®šï¼Ÿ",
        "å“ªäº›æ”¿ç­–æ–‡ä»¶æ¶‰åŠæ•™è‚²éƒ¨å…³äºä¸¾åŠ2015å¹´å…¨å›½èŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›çš„é€šçŸ¥ä»¥åŠå®´å¸­è®¾è®¡çš„ç›¸å…³è¦æ±‚ï¼Ÿ",
        '"å“ªäº›æ”¿ç­–æ–‡ä»¶æ¶‰åŠæ”¿åºœæ”¶æ”¯åˆ†ç±»ç§‘ç›®""1100245 æ•™è‚²å…±åŒè´¢æ”¿äº‹æƒè½¬ç§»æ”¯ä»˜æ”¶å…¥""å¹¶ç”¨äºæ”¯æŒç‰¹æ®Šæ•™è‚²äº‹ä¸šå‘å±•ï¼Ÿ"çš„æ„è§åœ¨æ•™è‚²èµ„æºé…ç½®æ–¹é¢æœ‰ä½•å…·ä½“è¡”æ¥ä¸å®æ–½ä¸¾æªï¼Ÿ',
        '"ç¬¬åä¸‰å±Š""æ¡ƒææ¯""å…¨å›½é’å°‘å¹´èˆè¹ˆæ•™è‚²æ•™å­¦æˆæœç°åœºå±•ç¤ºæ´»åŠ¨çš„é¢†é˜Ÿä¼šåŠæŠ½ç­¾ä¼šæ—¶é—´ä¸åœ°ç‚¹åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"',
        "ç¬¬äº”å±Šå…¨å›½å¤§å­¦ç”Ÿç½‘ç»œæ–‡åŒ–èŠ‚å’Œå…¨å›½é«˜æ ¡ç½‘ç»œæ•™è‚²ä¼˜ç§€ä½œå“æ¨é€‰å±•ç¤ºæ´»åŠ¨å…¥é€‰åå•çš„é€šçŸ¥æ˜¯ç”±è°åˆ¶å®šçš„ï¼Ÿ",
        "2023å¹´4æœˆ1æ—¥æ¨èæˆªæ­¢æ—¶é—´ä¸æ–‡åŒ–å’Œæ—…æ¸¸éƒ¨åŠå…¬å…å…³äºå®æ–½2023å¹´å…¨å›½æ–‡åŒ–è‰ºæœ¯èŒä¸šæ•™è‚²å’Œæ—…æ¸¸èŒä¸šæ•™è‚²æè´¨åŸ¹ä¼˜è¡ŒåŠ¨è®¡åˆ’çš„é€šçŸ¥ä¹‹é—´å­˜åœ¨ä½•ç§å…³ç³»ï¼Ÿ",
        "ç¬¬äºŒå±Šä¸­åç»å…¸è¯µå†™è®²å¤§èµ›çš„æˆæœå±•ç¤ºå®‰æ’ä¸æ•™è‚²éƒ¨åŠå…¬å…å…³äºä¸¾åŠç¬¬äºŒå±Šä¸­åç»å…¸è¯µå†™è®²å¤§èµ›çš„é€šçŸ¥ä¹‹é—´å­˜åœ¨ä½•ç§å…³ç³»ï¼Ÿ",
        "ç–«æƒ…é˜²æ§å¸¸æ€åŒ–èƒŒæ™¯ä¸‹ï¼Œ2021å¹´åŒç­‰å­¦åŠ›äººå‘˜ç”³è¯·ç¡•å£«å­¦ä½å…¨å›½ç»Ÿä¸€è€ƒè¯•å®‰å…¨å’Œé˜²ç–«å·¥ä½œçš„é€šçŸ¥å¦‚ä½•ç¡®ä¿è€ƒè¯•å®‰å…¨ä¸ç–«æƒ…é˜²æ§çš„åŒé‡ç›®æ ‡ï¼Ÿ",
        "å…¬å®‰éƒ¨å’Œæ•™è‚²éƒ¨è”åˆåˆ¶å®šçš„ä¸­å°å­¦å¹¼å„¿å›­å®‰å…¨é˜²èŒƒå·¥ä½œè§„èŒƒ(è¯•è¡Œ)å¯¹æ ¡å›­å®‰å…¨é˜²èŒƒå·¥ä½œæœ‰ä½•å…·ä½“å½±å“ï¼Ÿ",
        '"ç›´å±é«˜æ ¡åŸºæœ¬å»ºè®¾ä¿¡æ¯ç½‘æ˜¯æ•™è‚²éƒ¨åŠå…¬å…å…³äºç›´å±é«˜æ ¡å¼€å±•""åä¸‰äº”""åŸºæœ¬å»ºè®¾è§„åˆ’ç¼–åˆ¶å·¥ä½œçš„é€šçŸ¥æŒ‡å®šå¹³å°é…å¥—æ”¯æŒçš„ä¿¡æ¯åŒ–å¹³å°ã€‚"',
        "å“ªäº›é™¢æ ¡å¯ä»¥å‚ä¸2023å¹´å…¨å›½æ–‡åŒ–è‰ºæœ¯èŒä¸šæ•™è‚²å’Œæ—…æ¸¸èŒä¸šæ•™è‚²æè´¨åŸ¹ä¼˜è¡ŒåŠ¨è®¡åˆ’çš„ç”³æŠ¥ï¼Ÿ",
        '"æ•™è‚²éƒ¨å…³äºå°å‘2015å¹´å…¨å›½ç¡•å£«ç ”ç©¶ç”Ÿæ‹›ç”Ÿå·¥ä½œç®¡ç†è§„å®šçš„é€šçŸ¥ä¸çœ(åŒº,å¸‚)é«˜ç­‰å­¦æ ¡æ‹›ç”Ÿå§”å‘˜ä¼šåœ¨æ‹›ç”Ÿå·¥ä½œç®¡ç†ä¸­å­˜åœ¨ä½•ç§å…³ç³»ï¼Ÿ"',
        "æ¯åé€‰æ‰‹é™æŠ¥1åæŒ‡å¯¼æ•™å¸ˆçš„è§„åˆ™æ˜¯å¦é€‚ç”¨äºåŒä¸€å­¦æ ¡æŠ¥åäººæ•°ä¸è¶…è¿‡2äººçš„æƒ…å†µï¼Ÿ",
        "æ•™è‚²éƒ¨å…³äºåŒæ„ä¸­å›½åœ°è´¨å¤§å­¦æ±ŸåŸå­¦é™¢è½¬è®¾ä¸ºæ­¦æ±‰å·¥ç¨‹ç§‘æŠ€å­¦é™¢çš„å‡½ä¸­æåˆ°çš„æ¶‰åŠéƒ¨é—¨æ˜¯å¦åŒ…æ‹¬ä¸­å›½åœ°è´¨å¤§å­¦(æ­¦æ±‰)ï¼Ÿ",
        "äººåŠ›èµ„æºç¤¾ä¼šä¿éšœéƒ¨åŠå…¬å…å…³äºå¯ç”¨æ–°ç‰ˆæŠ€å·¥é™¢æ ¡æ¯•ä¸šè¯ä¹¦çš„é€šçŸ¥ä¸äººç¤¾å…å‡½[2022]76å·ä¹‹é—´å­˜åœ¨ä½•ç§å…³ç³»ï¼Ÿ",
        "ä¸“ä¸šæŠ€æœ¯äººæ‰çŸ¥è¯†æ›´æ–°å·¥ç¨‹2025å¹´é«˜çº§ç ”ä¿®é¡¹ç›®è®¡åˆ’çš„é€šçŸ¥ä¸­æåˆ°çš„é«˜çº§ç ”ä¿®é¡¹ç›®å¦‚ä½•é€šè¿‡ä¸“ä¸šæŠ€æœ¯äººæ‰çŸ¥è¯†æ›´æ–°å·¥ç¨‹å…¬å…±æœåŠ¡å¹³å°è¿›è¡Œç”³æŠ¥",
    ]


question_list = load_question_list()


@st.cache_resource
def load_policy_graph_rag() -> PolicyGraphRAG:
    working_dir = os.getenv("working_dir")
    llm_api_key = os.getenv("llm_api_key")
    llm_base_url = os.getenv("llm_base_url")
    llm_model_name = os.getenv("llm_model_name")

    llm = OpenAIProvider(
        config={
            "api_key": llm_api_key,
            "base_url": llm_base_url,
            "model_name": llm_model_name,
            "temperature": 0.3,
        }
    )
    embedding_type = os.getenv("embedding_type")
    if embedding_type == "huggingface":
        from policy_graphrag.embeddings.hugging_face import HuggingFaceEmbedding

        embedding_model = os.getenv("embedding_model")
        device = os.getenv("device")
        embed = HuggingFaceEmbedding(
            config={
                "device": device,
                "embedding_model": embedding_model,
            }
        )
    else:
        embedding_api_key = os.getenv("embedding_api_key")
        embedding_base_url = os.getenv("embedding_base_url")
        embedding_model_name = os.getenv("embedding_model_name")
        embed = OpenaiEmbedding(
            config={
                "api_key": embedding_api_key,
                "base_url": embedding_base_url,
                "model_name": embedding_model_name,
            }
        )

    pgr = PolicyGraphRAG(
        working_dir=working_dir, llm_provider=llm, embed_provider=embed
    )
    return pgr


pgr = load_policy_graph_rag()


# --- é…è‰²å·¥å…· ---
COLOR_PALETTE = [
    "#FF6B6B",
    "#4ECDC4",
    "#45B7D1",
    "#96CEB4",
    "#FFEEAD",
    "#FF9F76",
    "#A3C9A8",
    "#84A59D",
    "#F28482",
    "#679436",
    "#F7B267",
    "#2F4858",
]


class EnhancedColorAssigner:
    _color_map = {}

    @classmethod
    def get_color(cls, node_id: str) -> str:
        if node_id not in cls._color_map:
            hash_hex = hashlib.sha256(node_id.encode()).hexdigest()
            hash_int = int(hash_hex, 16)
            cls._color_map[node_id] = COLOR_PALETTE[hash_int % len(COLOR_PALETTE)]
        return cls._color_map[node_id]

    @classmethod
    def get_node_size(cls, degree: int, base_size: int = 20, scale: float = 2.0) -> int:
        return base_size + int(degree * scale)


# åˆå§‹åŒ– session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "search_result" not in st.session_state:
    st.session_state.search_result = {}


# --- ä¸‰æ å¸ƒå±€ ---
col1, col2, col3 = st.columns([0.5, 3, 2])


# ------------------- å·¦æ ï¼šé…ç½® -------------------
with col1:
    st.header("âš™ï¸ é…ç½®")

    # æŸ¥è¯¢å‚æ•°
    mode: Literal["graph", "naive"] = st.radio("æ¨¡å¼é€‰æ‹©", ["graph", "naive"], index=0)
    only_need_context: bool = st.checkbox("ä»…è¿”å›çŸ¥è¯†åº“", False)
    level: int = st.slider("æœç´¢å±‚çº§", 1, 5, 2)
    top_k: int = st.slider("Top-K", 1, 50, 10)
    threshold: float = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼", 0.0, 1.0, 0.7, 0.01)

    st.markdown("---")
    st.subheader("ğŸ“¥ æ·»åŠ æ”¿ç­–æ–‡ä»¶")
    uploaded_files = st.file_uploader(
        "æ·»åŠ æ”¿ç­–æ–‡ä»¶",
        label_visibility="hidden",
        accept_multiple_files=True,
        type=["txt", "md"],
        help="æ”¯æŒä¸Šä¼ å¤šä¸ªæ–‡æœ¬æ–‡ä»¶ï¼ˆ.txt, .mdï¼‰",
    )

    if st.button("æ·»åŠ åˆ°çŸ¥è¯†åº“"):

        async def upload_file():
            if uploaded_files:
                success_count = 0
                error_files = []
                with st.spinner(f"æ­£åœ¨å¤„ç† {len(uploaded_files)} ä¸ªæ–‡ä»¶..."):
                    for uploaded_file in uploaded_files:
                        try:
                            # ä» UploadedFile å¯¹è±¡ä¸­è·å–æ–‡ä»¶åå’Œå†…å®¹
                            filename = uploaded_file.name
                            # .read() è¿”å› bytes, éœ€è¦è§£ç ä¸º string
                            file_content = uploaded_file.read().decode("utf-8")

                            # ä¸ºæ¯ä¸ªæ–‡ä»¶è°ƒç”¨ index
                            await pgr.index(
                                policy_name=filename,
                                content=file_content,
                                is_update_community=False,
                            )
                            await pgr.index_naive(
                                policy_name=filename,
                                content=file_content,
                            )
                            success_count += 1
                        except Exception as e:
                            print_exception(e)
                            st.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {e}")
                            error_files.append(uploaded_file.name)

                if success_count > 0:
                    st.success(f"{success_count} ä¸ªæ–‡ä»¶å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“ï¼")
                if error_files:
                    st.warning(
                        f"{len(error_files)} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥: {', '.join(error_files)}"
                    )
            else:
                st.warning("è¯·å…ˆä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ–‡ä»¶ã€‚")

        asyncio.run(upload_file())

    st.markdown("---")
    st.subheader("ğŸ§© å›¾è°±æ˜¾ç¤ºé…ç½®")
    graph_config = {
        "width": st.slider("ç”»å¸ƒå®½åº¦", 500, 1500, 800),
        "height": st.slider("ç”»å¸ƒé«˜åº¦", 400, 1200, 600),
        "directed": st.checkbox("æ˜¾ç¤ºæ–¹å‘ç®­å¤´", True),
        "physics": st.checkbox("å¯ç”¨ç‰©ç†å¼•æ“", True),
    }


# ------------------- ä¸­æ ï¼šèŠå¤© -------------------
with col2:
    st.header("ğŸ’¬ å¯¹è¯")
    query_param = QueryParam(
        mode=mode,
        only_need_context=only_need_context,
        level=level,
        top_k=top_k,
        threshold=threshold,
    )

    # åˆ›å»ºç‹¬ç«‹çš„æ¶ˆæ¯æ˜¾ç¤ºå®¹å™¨
    chat_container = st.container(height=650)
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    # å°†è¾“å…¥æ¡†æ”¾åœ¨å®¹å™¨å¤–éƒ¨
    if prompt := st.chat_input("è¾“å…¥æ¶ˆæ¯..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with chat_container:
            with st.chat_message("user"):
                st.markdown(prompt)

        # å®šä¹‰ get_stream å‡½æ•°
        async def get_stream(result_msgs):
            async for item in pgr.query(prompt, query_param):
                msg = ""
                if "context" in item:
                    st.session_state.search_result["context"] = item["context"]
                if "context_report" in item:
                    st.session_state.search_result["context_report"] = item[
                        "context_report"
                    ]
                if "llm_response" in item:
                    msg = item["llm_response"]
                    result_msgs.append(msg)
                yield msg

        # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
        result_msgs = []
        with chat_container:
            with st.chat_message("assistant"):
                # write_stream éœ€è¦åœ¨ chat_message ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨
                st.write_stream(get_stream(result_msgs))

        # æ›´æ–°ä¼šè¯æ¶ˆæ¯
        st.session_state.messages.append(
            {"role": "assistant", "content": "".join(result_msgs)}
        )

    candidate_questions = random.sample(question_list, 5)
    st.markdown("ğŸ™‹æ¨èé—®é¢˜ï¼š")
    for i in range(len(candidate_questions)):
        st.markdown(f"{i+1}. {candidate_questions[i]}")


# ------------------- å³æ ï¼šä¸Šä¸‹æ–‡ -------------------
with col3:
    st.header("ğŸ” çŸ¥è¯†åº“")
    st.markdown("---")
    context = st.session_state.search_result.get("context")
    if context and mode == "naive" and isinstance(context, list):
        with st.expander("ğŸ“ æ–‡æœ¬ç‰‡æ®µ", expanded=True):
            for doc in context:
                st.markdown(f"- {doc}  \n")

    if context and mode == "graph" and isinstance(context, dict):
        st.subheader("ğŸŒ çŸ¥è¯†å›¾è°±")

        # --- ä¼˜åŒ–å¼€å§‹ ---
        async def _create_node_object(node_name, entity_type, description, degree):
            """è¾…åŠ©å‡½æ•°ï¼Œç”¨äºåˆ›å»º Node å¯¹è±¡"""
            return Node(
                id=node_name,
                title=f"{node_name}\nç±»å‹ï¼š{entity_type}\næè¿°ï¼š{description}",
                label=node_name,
                size=EnhancedColorAssigner.get_node_size(degree),
                color=EnhancedColorAssigner.get_color(entity_type),
                shape="dot",
            )

        async def build_graph():
            node_degrees, node_ids_from_edges = defaultdict(int), set()
            nodes, edges = [], []

            # 1. æ„å»º edges å¹¶è®¡ç®—åº¦æ•°
            for edge in context.get("edges", []):
                source, target = edge.source, edge.target
                if source and target:
                    node_ids_from_edges.update([source, target])
                    node_degrees[source] += 1
                    node_degrees[target] += 1
                    edges.append(
                        Edge(
                            source=source,
                            target=target,
                            label=edge.description,
                            color="#A0AEC0",
                        )
                    )

            # 2. å¤„ç†ä¸Šä¸‹æ–‡ä¸­å·²æœ‰çš„èŠ‚ç‚¹
            nodes_in_context = {}
            for node in context.get("nodes", []):
                nodes_in_context[node.name] = node
                nodes.append(
                    await _create_node_object(
                        node_name=node.name,
                        entity_type=node.entity_type,
                        description=node.description,
                        degree=node_degrees[node.name],
                    )
                )
                # å¦‚æœèŠ‚ç‚¹å·²å¤„ç†ï¼Œä»å¾…æŠ“å–é›†åˆä¸­ç§»é™¤
                if node.name in node_ids_from_edges:
                    node_ids_from_edges.remove(node.name)

            # 3. æŠ“å–å‰©ä½™çš„ã€ä»…åœ¨è¾¹ä¸­å‡ºç°çš„èŠ‚ç‚¹
            for node_id in node_ids_from_edges:
                node = await pgr.get_node(entity_name=node_id)
                if node:
                    nodes.append(
                        await _create_node_object(
                            node_name=node.name,
                            entity_type=node.entity_type,
                            description=node.description,
                            degree=node_degrees[node.name],
                        )
                    )

            # 4. æ¸²æŸ“å›¾è°±
            if nodes and edges:
                config = Config(**graph_config)
                agraph(nodes=nodes, edges=edges, config=config)
            else:
                st.info("æœªæ‰¾åˆ°å¯æ˜¾ç¤ºçš„å›¾è°±æ•°æ®ã€‚")

        try:
            asyncio.run(build_graph())
        except RuntimeError as e:
            if "cannot run" in str(e):
                st.error("å›¾è°±æ„å»ºå¼‚æ­¥é”™è¯¯ï¼šStreamlit äº‹ä»¶å¾ªç¯å†²çªã€‚è¯·å°è¯•åˆ·æ–°é¡µé¢ã€‚")
            else:
                st.error(f"å›¾è°±æ„å»ºå¤±è´¥: {e}")
        # --- ä¼˜åŒ–ç»“æŸ ---

        st.markdown("---")

        cites = pgr.get_cites_policy(context)
        if cites:
            with st.expander("ğŸ“œ ç›¸å…³æ”¿ç­–", expanded=True):
                for cite in cites:
                    st.markdown(f"- {cite}")
        else:
            st.markdown("æ— ç›¸å…³æ”¿ç­–")

        context_report = st.session_state.search_result.get("context_report")
        if context_report:
            # ä½¿ç”¨ st.expander æ¥èŠ‚çœç©ºé—´
            with st.expander("ğŸ“ ä¸Šä¸‹æ–‡æŠ¥å‘Š"):
                st.markdown(context_report)
