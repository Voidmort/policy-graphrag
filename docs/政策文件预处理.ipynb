{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "120a1f11",
   "metadata": {},
   "source": [
    "# 政策法规提取和预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee76ef",
   "metadata": {},
   "source": [
    "政策法规文件来自**北大法宝**。为了便于后续的知识库构建和问答系统开发，需要对这些文件进行预处理，包括解压缩、文本提取和清洗等步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a80ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def unzip_all(src_dir: str, dst_dir: str):\n",
    "    \"\"\"\n",
    "    解压src_dir目录下的所有zip文件，并把内容移动到dst_dir目录\n",
    "    解决中文文件名乱码问题\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(src_dir):\n",
    "        if file.lower().endswith(\".zip\"):\n",
    "            zip_path = os.path.join(src_dir, file)\n",
    "            print(f\"正在解压: {zip_path}\")\n",
    "\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "                    for info in zf.infolist():\n",
    "                        # 解决中文乱码\n",
    "                        try:\n",
    "                            filename = info.filename.encode('cp437').decode('utf-8')\n",
    "                        except UnicodeDecodeError:\n",
    "                            filename = info.filename.encode('cp437').decode('gbk', errors='ignore')\n",
    "\n",
    "                        # 目标路径\n",
    "                        extracted_path = os.path.join(dst_dir, filename)\n",
    "\n",
    "                        # 如果是目录，创建\n",
    "                        if info.is_dir():\n",
    "                            os.makedirs(extracted_path, exist_ok=True)\n",
    "                        else:\n",
    "                            # 确保父目录存在\n",
    "                            os.makedirs(os.path.dirname(extracted_path), exist_ok=True)\n",
    "                            with zf.open(info) as source, open(extracted_path, \"wb\") as target:\n",
    "                                shutil.copyfileobj(source, target)\n",
    "\n",
    "                print(f\"完成解压: {file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"解压失败 {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f82ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"./政策zip\"\n",
    "target_directory = \"./政策\"\n",
    "unzip_all(source_directory, target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "HTML到Markdown转换器\n",
    "该脚本用于解析特定格式的HTML文件，提取元数据和正文内容，\n",
    "并将内容转换为Markdown格式。\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "\n",
    "def parse_html_to_structured_data(html_string: str) -> dict:\n",
    "    \"\"\"\n",
    "    解析给定的HTML字符串，提取元数据和转换为Markdown的正文内容。\n",
    "    Args:\n",
    "        html_string: 包含法律文档的HTML字符串。\n",
    "\n",
    "    Returns:\n",
    "        一个包含解析后数据的字典。\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_string, 'lxml')\n",
    "    result_data = {}\n",
    "    \n",
    "    field_mapping = {\n",
    "        '制定机关': 'issuing_authority',\n",
    "        '发文字号': 'document_number',\n",
    "        '公布日期': 'publish_date',\n",
    "        '施行日期': 'effective_date',\n",
    "        '时效性': 'timeliness',\n",
    "        '效力位阶': 'effectiveness_level',\n",
    "        '法规类别': 'category'\n",
    "    }\n",
    "\n",
    "    # --- 提取元数据 ---\n",
    "    title_tag = soup.find('h2', class_='title')\n",
    "    if title_tag:\n",
    "        result_data['title'] = title_tag.get_text(strip=True)\n",
    "\n",
    "    fields_container = soup.find('div', class_='fields')\n",
    "    if fields_container:\n",
    "        for li in fields_container.find_all('li'):\n",
    "            box = li.find('div', class_='box')\n",
    "            if not box: continue\n",
    "            strong_tag = box.find('strong')\n",
    "            if not strong_tag: continue\n",
    "            field_name_cn = strong_tag.get_text(strip=True).replace('：', '')\n",
    "            field_name_en = field_mapping.get(field_name_cn)\n",
    "            \n",
    "            if field_name_en:\n",
    "                if field_name_en == 'category':\n",
    "                    values = [a.get_text(strip=True) for a in box.find_all('a')]\n",
    "                    result_data[field_name_en] = ' '.join(values)\n",
    "                else:\n",
    "                    value_tag = box.find('a')\n",
    "                    if value_tag:\n",
    "                        result_data[field_name_en] = value_tag.get_text(strip=True)\n",
    "                    else:\n",
    "                        full_text = box.get_text(strip=True)\n",
    "                        value = full_text.replace(strong_tag.get_text(strip=True), '', 1).strip()\n",
    "                        result_data[field_name_en] = value\n",
    "\n",
    "    # --- 处理正文内容并转换为Markdown ---\n",
    "    content_div = soup.find('div', id='divFullText')\n",
    "    \n",
    "    if content_div:\n",
    "        # 在转换为Markdown之前，遍历所有<a>标签\n",
    "        # 并用标签内的文本替换掉整个<a>标签。\n",
    "        # 例如: <a href=\"...\">文本</a> 会直接变成 \"文本\"\n",
    "        for a_tag in content_div.find_all('a'):\n",
    "            a_tag.replace_with(a_tag.get_text())\n",
    "        # ----------------------------------------------------\n",
    "            \n",
    "        # 现在 content_div 里的HTML已经没有<a>标签了，可以直接转换\n",
    "        markdown_content = md(str(content_div), heading_style=\"ATX\", strip=['br'])\n",
    "\n",
    "        lines = markdown_content.splitlines()\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            # 移除可能由markdownify产生的多余转义或格式问题\n",
    "            cleaned_line = line.rstrip()\n",
    "            # 移除由<p><br></p>等产生的空行 (如果markdownify未处理好)\n",
    "            if not (cleaned_line.isspace() or cleaned_line == \"\") or (cleaned_lines and not (cleaned_lines[-1].isspace() or cleaned_lines[-1] == \"\")):\n",
    "                 cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        # 进一步清理，移除开头和结尾的空行\n",
    "        while cleaned_lines and (cleaned_lines[0].isspace() or cleaned_lines[0] == \"\"):\n",
    "            cleaned_lines.pop(0)\n",
    "        while cleaned_lines and (cleaned_lines[-1].isspace() or cleaned_lines[-1] == \"\"):\n",
    "            cleaned_lines.pop()\n",
    "\n",
    "        markdown_content = \"\\n\".join(cleaned_lines)\n",
    "        \n",
    "        result_data['content_markdown'] = markdown_content.strip().replace('\\u3000', '')\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 遍历文件夹下的所有文件\n",
    "input_folder = target_directory\n",
    "all_parsed_items = []\n",
    "\n",
    "for file_name in tqdm(os.listdir(input_folder)):\n",
    "    if file_name.endswith('.html'):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        try:\n",
    "            # 读取HTML文件内容\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "\n",
    "            # 解析HTML内容\n",
    "            parsed_items = parse_html_to_structured_data(html_content)\n",
    "            all_parsed_items.append(parsed_items)\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 '{file_name}' 时发生错误: {e}\")\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(all_parsed_items)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb64c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 数据处理和按年统计 ---\n",
    "# 1. 将 'publish_date' 列转换为字符串类型（以防万一）\n",
    "df['publish_date'] = df['publish_date'].astype(str)\n",
    "df['effective_date'] = df['effective_date'].astype(str)\n",
    "\n",
    "# 2. 定义一个函数来解析不规则的日期字符串\n",
    "def parse_incomplete_date(date_str):\n",
    "    \"\"\"尝试将 'YYYY.MM.DD', 'YYYY.MM', 或 'YYYY' 格式的字符串解析为 datetime 对象\"\"\"\n",
    "    formats = ['%Y.%m.%d', '%Y.%m', '%Y'] # 尝试的日期格式列表\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            # 对于只有年份的情况，pandas.to_datetime 默认会将月份和日期设为 1月1日\n",
    "            return pd.to_datetime(date_str, format=fmt, errors='raise')\n",
    "        except ValueError:\n",
    "            continue # 如果当前格式不匹配，则尝试下一个格式\n",
    "    # 如果所有格式都失败，则返回 NaT (Not a Time)\n",
    "    return pd.NaT\n",
    "\n",
    "# 3. 应用函数解析日期列\n",
    "df['publish_date'] = df['publish_date'].apply(parse_incomplete_date)\n",
    "df['effective_date'] = df['effective_date'].apply(parse_incomplete_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据\n",
    "df.to_parquet(\"政策法规.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zcllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
